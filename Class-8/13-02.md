---
title: Class 8
date: February 13, 2023
---

# Recap

Recall $x_1, x_2, \ldots x_n \sim p$ 

Can construct a density estimate $\hat{p}_\theta(x) \approx p$.
We can then sample from $y \sim \hat{p}_\theta (x)$ to generate.

This is one approach of generation.

But GAN uses two distributions and we'll see how that follows.

So, at the EOD we want to measure $\hat{p}_\theta(x)$ wrt $p$. **So how to measure two distributions?**

Some papers that highlight some measures:

- Adhikari and Joshi 1956 "measures of distance"
- Rao 1952, "measures of separations"
- Chernoff and Kullback, "measures of discriminatory intent"
- Kolmogorov 1963, "measures of variation distance"

\begin{example}
  
  Two-norm $\| x - y\|^2_2$ preserves differentiability. While one-norm $\|x - y\|_1$ preserves sparsity.

  Thus, we see different measures measure (and quantify) different sort of information.

  Another way to measure is to divide the PMF at each point.

  $$
  \phi(x) = \frac{p_1(x)}{p_2(x)} 
  $$ 

  Then we can take $\log$ of this which will be zero when both distributions are equal.
  We can go further and take expectation of this $E_{x \sim p_1} \log \frac{p_1(x)}{p_2(x)}$.

\end{example}

\begin{remark}
 A common property for each measure is that they \textbf{increase} as two distributions \textbf{more apart}.
\end{remark}

# Kullback-Liebler (KL) measure

$$
D(p \| p_\theta) = \sum_x p(x) \log \frac{p(x)}{p_\theta(x)} = E_{x\sim p} \log \frac{p(x)}{p_\theta(x)}
$$ 

KL measure is very equivalent to say a loss function. Whatever $p_\theta$ we get, we want to minimize this measure.

## Minimize KL divergence

\begin{align*}
  \inf_\theta D(p \| p_\theta) &= \inf_\theta \sum_x p(x) \log \frac{p(x)}{p_\theta(x)}\\
  &= \inf_\theta \sum_x p(x) \log \frac{1}{p_\theta(x)} - \sum_x p(x) \log \frac{1}{p(x)}\\
  & \textit{Ignore the 2nd term because it's not dependent on } \theta \\
  &= \inf_\theta \bp{ \sum_x p(x) \log \frac{1}{p_\theta(x)} }\\
  &= \sup_\theta \bp{ \sum_x p(x) \log p_\theta(x) } \\
  &= \sup_\theta E_{x\sim p} \log p_\theta(x) \textit{ which is the log-MLE estimate}
\end{align*}

## Some properties of KL-divergence

1. KL-Divergence is convex in the pair $(p,q)$

**Claim:**
  $$
  KL(\lambda p_1 + (1 - \lambda) p_2 \| \lambda q_1 + (1 - \lambda) q_2) 
  \leq \lambda KL(p_1 \| q_1) + (1 - \lambda) KL(p_2 \| q_2) 
  $$ 

\begin{proof}
  We use the log-sum inequality (exercise: try to prove),

  $$
  \sum_{i=1}^n a_i \log \frac{a_i}{b_i} \ge \bp{\sum_{i=1}^n a_i} \log \frac{\sum_{i=1}^n a_i}{\sum_{i=1}^n b_i}
  $$ 

  \begin{align*}
      & KL(\lambda p_1 + (1 - \lambda) p_2 \| \lambda q_1 + (1 - \lambda) q_2)\\
      &= \sum_x \bs{\bp{\lambda p_1(x) + (1 - \lambda) p_2(x)} \log \frac{\lambda p_1(x) + (1 - \lambda) p_2(x)}{\lambda q_1(x) + (1 - \lambda) q_2(x)}  }\\
      & \textit{Take } a_1 = \lambda p_1, a_2 = (1 - \lambda) p_2\\
      & \textit{and } b_1 = \lambda q_1, b_2 = (1 - \lambda) q_2\\
      & \textit{On applying log sum ineqality we have}\\
      &\leq \sum \bs{ \lambda p_1(x) \log \frac{p_1(x)}{q_1(x)} + (1 - \lambda ) p_2(x) \log \frac{p_2(x)}{q_2(x)} }\\
      &= \lambda KL(p_1 \| q_1) + (1 - \lambda) KL(p_2 \| q_2) 
  \end{align*}

\end{proof}

### Example

|$x$|0|1|2|
|-|-|-|-|
|$p(x)$|$9/25$|$12/25$|$4/25$|
|$q(x)$|$1/3$|$1/3$|$1/3$|

Now compute $D_{KL} (p\|q)$.

## Properties (contd.)

KL-divergence is not a **metric**.

- Since not commutative
- Triangle Ineqality not satisfied (exercise: check)

# Generalize KL-divergence

Now we will generalize KL to some variational approach.

$$
D(p \| q) = \sum_x p(x) \log \frac{p(x)}{q(x)} = \sum p(x)  \bc{ - \log \frac{q(x)}{p(x)}}
$$ 

So, we can replace $- \log$ with some other convex function $f$ such that $f(1) = 0$.

## f-divergence

Introduced by "A general class of coefficients of divergence of one distribution from another" by S. M. Ali, S. Silvey (1965).

\begin{definition}
  Let $f: R \ra R$, convex lower-semicontinuous such that $f(1) = 0$. We define f-divergence between two densities $p$ and $q$ on $X$ as 

  $$
  D_f(p\|q) = \int_X q(x) f \bp{\frac{p(x)}{q(x)}} dx 
  $$ 
\end{definition}

KL-divergence is a special case of F-divergence. Take $f(x) = x \log x$ to show that.

\begin{definition}
   $f$ is lower-semicontinuous (lsc) if and only if 
  $$
  \lim_{x\ra x_0} f(x) \ge f(x_0)
  $$ 
\end{definition}

![Lower Semicontinuity](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Lower_semi.svg/330px-Lower_semi.svg.png){width=40%}

### Equivalent definitions for lsc

- $\{x \in X, f(x) \le y\}$ Sub level sets are closed in $X$ 
- Epigraph is closed in $X$ 

(exercise) Write two functions that are both lsc and convex? Can we have a function that is cocave but lsc?

## Tractable f-divergence

The integral is usually difficult to solve. So another researcher proposed this measure. Essentially we construct a lower bound that is tractable

## Variational representation of f-divergence

> Conjugate function revisited

(exercise)

- Find conjugate for $f(x) = ax+ b$. Comes out to be $f^*(t) = -b$.
- Find conjugate for $f(x) = x \log x$. Comes out to be $f^*(t) = e^{t-1}$.

### Conjugate of conjugate

\begin{theorem}[Fenchel's - Moreau Theorem]
If $f$ is closed and a convex function then $f^{**} = f$. 
\end{theorem}

Proof can be taken as a project and found in the Rockafeller Convex analysis 1970 book.

## Variational Approach

Proposed by Nguyen, Jordan 2010.

**Claim**: $D_f(p\|q) = \sup_{T: X\ra R} E T(x) - E_{x\sim p} f^*(T(x))$ 

Using variational representation, $f(x) = sup_t \bc{ tx - f^*(t) }$.


