---
title: Class 5 | Random Vector and Matrices 
date: January 23, 2023
---

# Notation

Given $n$ random variables $X_1, X_2, \ldots, X_n$,

$$X = 
\begin{bmatrix}
  X_1 \\
  X_2\\
  \vdots \\
  X_n
\end{bmatrix}
$$

Here, $X$ is a called a $n$-dimensional **random vector**.

- $F_X(x) = F_{X_1, X_2,\ldots, X_n} (x_1, x_2, \ldots, x_n)$
- In a similar way we can define PDF.
- Expectation $E[X] = 
\begin{bmatrix}
  E[X_1] & E[X_2] & \cdots & E[X_n]
\end{bmatrix}^\top$

Similarly we can extend to a ***random matrix***, $M_{m\times n}$ and generalize
expectation for $M$ (expectation of each entry in the matrix).

# Correlation and Covariance

Take $R_X = E[XX^\top]$ (outer product).

$$
R_X = E\left[
\begin{bmatrix}
    X_1 \\
  X_2\\
  \vdots \\
  X_n
\end{bmatrix}
\begin{bmatrix}
    X_1 & X_2 & \cdots & X_n
\end{bmatrix}
\right]
$$

where $[R_X]_{ij} = E[X_i X_j]$.

## Covariance Matrix

$C_X = E[(X - E[X]) (X - E[X])^\top]$.

$$\therefore [C_X]_{ij} = Cov(X_i, X_j)$$ 

\begin{remark}
$C_X = R_X - E[X] E[X^\top]$
\end{remark}

\begin{remark}
$X$ is $n$-dimensional random vector and $Y = AX + b,\ A\in R^{m\times n}, 
b\in R^m$.

$$
\therefore C_Y = A C_X A^\top
$$ 
\end{remark}

### Properties of Covariance Matrix

- $C_X$ is symmetric 
- $C_X$ is positive definite 

\begin{example}
  $X, Y$ are independent $U(0,1)$ random vectors. $U = [X \quad X+Y]^\top$ and 
$V = [X \quad Y\quad X+Y]^\top$.

Determine whether $C_U$ and $C_V$ are positive definite.

\textbf{Solution:}

$Var(X) = Var(Y) = \frac{1}{12}$. And $Cov(X, X+Y) = \frac{1}{12}$ and 
$Var(X+Y) = \frac{1}{6}$.

Then, compute $C_U$ and $C_V$.

\end{example}

## Cross-Correlation

Define $R_{XY} = E[XY^\top]$, then cross correlation can be defined as:
$$
C_{XY} = E[(X - E[X])(Y - E[Y])^\top]
$$ 

# Method of Trasform for Random Vectors

- $X := n$-dimensional random vector with PDF $f_X(x)$
- $G: R^n\rightarrow R^n$ be a continuous and invertible function with
continuous partial derivative and let $\mathcal{H}=G^{-1}$.
- $Y = G(X) \implies X = \mathcal{H}(Y)$.

  PDF of $Y$ is thus, given as
  $$
  f_Y(y) = f_X(\mathcal{H}(y))\ |J|
  $$
  where $J$ is the Jacobian. $J_{ij} = \frac{\partial\mathcal{H}_i}{\partial y_j}$. 


\begin{example}

$A$:= fixed invertible $n$ by $n$ matrix and $b$:= fixed, $n$-dimensional vector.

Define $Y = AX +b$. Find PDF of $Y$.

\textbf{Solution:}

$X = A^{-1} (Y - b) = \mathcal{H}(Y)$.

$J =  \frac{\partial\mathcal{H}}{\partial Y} = A^{-1}$

$\therefore |J| = \det(A^{-1}) = \frac{1}{\det(A)}$.

$f_Y(y) = f_X(A^{-1}(y - b)) \frac{1}{\det(A)}
$

\end{example}

# Standard Normal Vector

Let $X_i \sim N(0,1)$, assume $X_i$'s are iid.

Denote standard Normal Vector $Z = [Z_1 \quad Z_2 \quad \cdots \quad Z_n]^\top$
where $Z_i$'s are iid and standard normal. Then, 
$f_Z(z) = \prod_i f_{Z_i}(z_i)$.

**Recall**, $X = \sigma Z + \mu$ then $X\sim N(\mu, \sigma^2)$.

Normal Random vector $X$ with mean vector $m$ and covariance matrix $C$.

Let $Z \sim N(0,I)$ and $X = AZ + m$ where $A A^\top = A^\top A = C$.

**Claim**: $X\sim N(m,C)$.

\begin{proof}
$E[X] = E[AZ + m] = A E[Z] + m = 0 + m = m$

Since, $C$ is symmetric and positive definite. We can do eigenvalue decomposition.
Since, $C$ is symmetric, it is guaranteed to have $n$-independent (orthogonal)
eigenvalues.

$C = Q D Q^\top$ where columns of $Q$ are eigenvectors of $C$ and $D$ is a 
diagonal matrix (with the eigenvalues).

$C Q = QD \implies C q_i = \lambda_i q_i$.

Now, define $A$ as $Q D^{1/2} Q^\top$

$$C = A A^\top = (Q D^{1/2} Q^\top) (Q D^{1/2} Q^\top) = QDQ^\top$$

Note, $D^{1 / 2}$ always exists because all eigenvalues are $\ge 0$.

\begin{align*}
f_X(x) &= \frac{1}{\det A} f_Z(A^{-1} (X - m))\\
&= \frac{1}{(2\pi)^{n / 2} (\det A)} \exp \left\{ -\frac{1}{2} (A^{-1} (X - m))^\top (A^{-1}(X - m)) \right\}\\
&= \frac{1}{(2\pi)^{n / 2} |\det A|} \exp \left\{ -\frac{1}{2}  (X - m)^\top C^{-1} (X - m) \right\}
\end{align*}

\end{proof}

# Probability Bounds

## Union Bound, Bode's inequality

For any $A_1, A_2, \cdots, A_n$

$$
P(\cup_i A_i) \leq \sum_i P(A_i)
$$ 

\begin{proof}
  By PIE,

$$
P(\cup A_i) = \sum P(A_i) - \sum P(A_i \cap A_j) + \sum P(A_i \cap A_j \cap A_k)
+\ldots + (-1)^{n-1} P(\cap_{i=1}^n A_i)
$$ 

Thus, $P(\cup_i A_i) \leq \sum_i P(A_i)$ and $P(\cup_i A_i) \geq \sum_i P(A_i)
-  \sum P(A_i \cap A_j) $ and so on.

\end{proof}

## Markov Inequality

$X:=$ positive continuous random variable

\begin{align*}
E[X] &= \int_0^\infty x f_X(x) dx\\
&\geq \int_a^\infty x f_X(x) dx\\
&\geq \int_a^\infty a f_X(x) dx\\
&= a \int_a^\infty f_X(x) dx\\
&= a P(X \ge a)\\
&\\
\implies P(X\ge a) &\leq  \frac{E[X]}{a}, \quad a > 0
\end{align*}

### Chebyshev's Inequality

Define $Y = (X - E[X])^2$, $X$ is any random variable. $Y$ is therefore a non-negative
random variable.

Applying Markov Inequality on $Y$,

\begin{align*}
P(Y\geq b^2) &\leq \frac{E[Y]}{b^2} = \frac{Var(X)}{b^2}\\
\implies P((X - E[X])^2 \geq b^2) &\leq \frac{Var(X)}{b^2}\\
\implies P(|X - E[X]| \geq b) &\leq \frac{Var(X)}{b^2}\\
\end{align*}

## Cauchy Schwarz Inequality

For any two random variables $X$ and $Y$
$$
\left| E[XY] \right| \leq \sqrt{E[X^2] E[Y^2]}
$$ 

where equality holds iff $X = \alpha Y, \ \alpha \in R$

## Jensen's Inequality

Take a convex function $f(x)$ then

$$
E[f(X)] \geq f(E[X])
$$

## Law of Large Numbers

For iid random variables $X_1, X_2,\ldots, X_n$, the sample mean denoted by 
$\overline{X} = \sum X /n$.

Let $E[X_i] = \mu < \infty$ (finite mean).

Then, for any $\epsilon > 0$,

$$
\lim_{n \rightarrow \infty} P(|\overline{X} - \mu| \ge \epsilon) = 0
$$ 
\begin{proof}
  Assume $Var(X) = \sigma^2$.
  
  By Chebyshev's inequality,

\begin{align*}
P(|\overline{X} - \mu| \geq \epsilon) &\leq \frac{Var(\overline{X})}{\epsilon^2}\\
&= \frac{Var(X)}{n\epsilon^2} \rightarrow 0 \text{ as } n\rightarrow \infty
\end{align*}
  
\end{proof}

## Central Limit Theorem

- $X_i:=$ iid
- $E[X_i] = \mu < \infty$
- $Var(X_i) = \sigma^2 < \infty$
- Take sample mean $\overline{X}$.

Take Normalized random vector 
$$
Z_n = \frac{\overline{X} - \mu}{\sigma / \sqrt{n}}
$$ 

$Z_n$ converges in distribution to the standard normal random variable i.e.,

$$
\lim_{n\rightarrow \infty} P(Z_n \leq x) = \Phi(x) \quad \forall x \in R
$$ 



