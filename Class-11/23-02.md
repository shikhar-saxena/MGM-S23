---
title: Class 11
date: February 20, 2023
---

# WGAN Training Algorithm

Refer from the [WGAN-Paper](https://arxiv.org/pdf/1701.07875v3.pdf)

There's many Descent Algorithms. Some examples are SGD, RMSProp, ADAM, etc.

WGAN (the one stated in the paper) was implemented using RMSProp Optimizer (because Adam didn't perform well and training became unstable when they used it).

We clip the weights to guarantee the K-Lipschitz condition (not a very good way but is simplistic and performed well for the authors).

# WGAN comparison with GAN

- WGAN does not require maintaining a careful balance in training the discriminator and generator (not very sensitive to $n_{critic}$).
- WGAN reduces mode dropping and mode collapse which is an improvement over GANs.


