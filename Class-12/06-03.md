---
title: Class 12
date: March 06, 2023
---

# Recap

- Generative Modeling
- GANs
  - KL and f divergence
  - bring sup out of integral (variational approach)
  - $E_p[f(x)] - E_q[f(y)]$ 
- WGANs
  - Wasserstein distance W-1 $\inf_{\gamma ~\Pi(p,q)} E\bs{ \ \|x - y \| \ }$ 
  - KR Duality $\sup_{\|h\| \le 1} E[h(x)] - E[h(y)]$ 
  - Gradient Penalty in WGAN improves performance (by enforcing the 1-Lipschitz condition better). $\sup_{\|h\|_2 \le 1} E[h(x)] - E[h(y)] + (\|\nabla h \| - 1)^2$


\begin{proof}[Gradient Penalty Proof]

There exists an $f^*$ that is a solution to the KR duality problem (optimal minimizer for W-1 distance) such that

$$
\nabla f^* (x_t) = \frac{y - x_t}{\|y - x_t\|} \implies \|\nabla f^*(t)\| = 1
$$

Here $v$ is the direction in which $\frac{\partial f}{\partial v} = 1$. 

Since $f^*$ is 1-Lipschitz $\implies \|\nabla f^*(x) \| \le 1$. 

$$
 \|\nabla f^*(x_t)\|^2 = \langle v, \nabla f^* (x_t)\rangle^2 + \|\nabla f^*(x_t) - \langle v, \nabla f^*(x_t) v \rangle\|^2\\
$$ 

Proof:
\begin{align*}
  1 &\ge \|\nabla f^*(x_t)\|^2\\
  &= \langle v, \nabla f^* (x_t)\rangle^2 + \|\nabla f^*(x_t) - \langle v, \nabla f^*(x_t) v \rangle\|^2\\
  &= \langle v, \nabla f^* (x_t)\rangle^2 + \|\nabla f^*(x_t)\|^2 + \| \langle v, \nabla f^*(x_t) \rangle v\|^2  - 2 \langle \nabla f^*(x_t), \langle v, \nabla f^*(x_t)\rangle v \rangle\\ 
\end{align*}
(Some terms cancel) and we get that expression.
\end{proof}

Thus, we get that $\nabla f^*(x_t) = v$ (since norm greater than and less than equal to 1).

# Optimal Transport

$(X,p)$ and $(Y,q)$ be some finite probability space with $|X| = n$ and $|Y| = m$.

$\Pi(p,q)$: Collection of distributions on the product $X\times Y$ with marginals $p$ and $q$ 

Consider a cost $C: X\times Y \ra R_+$ then the optimal transport problem is:

\begin{equation}
d_c(p,q) = \min_{\pi \in \Pi(p,q)} \inn{C}{\pi} = \min_{\pi} \sum_{x,y} C(x,y)\pi(x,y)
\label{eq:ot}
\end{equation}

## Example

  If $X,Y \subset R^n, \ C(x,y) = \|x - y\|_2$ then optimal transport between $p$ and $q$:

  $$
  d_c(p,q) = \min_\pi \sum_{x,y} \|x-y\|_2 \pi(x,y) = W_1(p,q)
  $$ 
Marginals: 

  $p(x) = \sum_{i=1}^m \pi(x,y_i) = (\pi \ 1_m)_x$ where $1_m$ is vector of all ones.

  Similarly, $q(y) = \sum_{i=1}^n \pi(x_i,y) = 1_n^T \pi$.

Hence \eqref{eq:ot} simplifies to

\begin{equation}
d_c(p,q) = \min_{\pi 1_m = p, \pi^T 1_n = q} \sum_{x,y} c(x,y) \pi(x,y)
\label{eq:d}
\end{equation}

\begin{remark}
  Discrete EM distance minibatch approximation of W-distance between two continuous distributions.
\end{remark}

\underline{Concretely}, If $p$ and $q$ are continuous distribution. 

$\{x_i\} \sim p$ and $\{y_i\}\sim q$ 

Then, $\tilde{p}(x) = 1/n \sum_{i=1}^n 1_{x_i = x}, \tilde{q}(y) = 1/m \sum_{i=1}^m 1_{y_i = y}$ 


Expect that $W_1(p,q) \approx W_1(\tilde{p}, \tilde{q})$ 

Recall LP and that \eqref{eq:d} is a LP. We can use LP solvers.

# Sinkhorn Generative Modeling

- More tractable that WGAN

\underline{Key Idea}: Sinkhorn Distance (*Cuturi, 2013, Sinkhorn Distance. Lightspeed computation of optimal transport.*)

He added a term to the optimization problem \eqref{eq:d},

\begin{equation}
  d_c^\lamb(p,q) = \min_{\pi \in \Pi} \inn{C}{\pi} - \lambda H(\pi)
  \label{eq:f}
\end{equation}

Clearly as $\lamb \ra 0, \ d^*_c(p,q) \ra d_c(p,q)$. Reframe \eqref{eq:f} as a KL divergence.

- Define $K(x,y) = e^{-c(x,y)/\lamb}$.
- Let $Z_\lamb  = \sum_{x,y} K(x,y)$.

Then $\frac{1}{Z_\lamb} K(x,y)$ defines a Gibb's distribution $p_k^\lamb$.

We have
$$
D_{KL}(\pi \| p_k^\lamb) = \sum_{x,y} \pi(x,y) \log \bp{ \frac{\pi(x,y)}{K(x,y)} Z_\lamb }
$$ 

\begin{align*}
D_{KL}(\pi \| p_k^\lamb) &= \sum_{x,y} \pi(x,y) \log \pi(x,y) - \sum_{x,y} \pi(x,y) \bp{-  \frac{c(x,y)}{\lamb }} + \sum_{x,y} \pi(x,y) \log Z_\lambda \\
&= -H(\pi) + \frac{1}{\lamb} \inn{C}{\pi} + \log Z_\lamb
\end{align*}

Therfore, 

\begin{equation}
arg\min_{\pi \in \Pi} \inn{C}{\pi} - \lamb H(\pi) = arg\min_\pi D(\pi \| p_k^\lamb)
  \label{eq:star}
\end{equation}

## Questions for solver for \eqref{eq:star}

1. \underline{Existence}: Does a solution exist? 
2. \underline{Uniqueness}: Is the solution unique?

Recall, from Metric Space Calculus and topology,

- A continuous function on a compact space attains extrama
- \underline{Claim 1} $\Pi(x,y)$ is a compact space
- \underline{Claim 2} $-H(\pi)$ is strongly convex

\begin{definition}[$-mu$ strongly convex]
A function is strongly convex if 

$$
\nabla^2 f(x) \ge \mu I 
$$ 

(same as $\lamb_{min} (\nabla^2 f) \ge \mu > 0$). This implies $f$ is $\mu$-strongly convex.
\end{definition}

\begin{proof}[For Claim 2]
  $$
  \nabla^2 H(\pi) = 1/\pi > 0
  $$ 
  Hence strongly convex. Take any $\mu < 1$, then $H(\pi)$ will be $\mu$-strongly convex.
\end{proof}


\begin{proof}[For Claim 1]
If $\Pi$ is compact ($X$ and $Y$ is compact) then $\pi(p,q)$ is a \textbf{closed subset} of $\Pi$. This follows from closed subset of compact set is compact.

Also, Convexity implies continuity. So continuous functions on a compact space attain minima.

Strong Convexity implies uniqueness of solutions.

So, for some $f:X\ra Y$ such that $f(X)$ is compact and is closed and bounded.
\end{proof}


$\inn{C}{\pi}$ being linear and $-\lamb H(\pi)$ being strongly convex, then $\inn{C}{\pi} - \lamb H(\pi)$ is strongly convex.

## Conclusion 

\eqref{eq:f} has existence of solution and solution is unique.

$$
arg\min_\pi D(\pi \| p_k^\lamb)
$$ 

Let $\Pi(p)$ and $\Pi(q)$ denote row and column marginal constraints. And another constraint that $\Pi(p,q) = \Pi(p) \cap \Pi(q)$. So we want to find intersection point for two convex sets here (which is a generic optimization problem).

This can be solved using alternating projection.

## Alternating Projection

Refer [here](https://web.stanford.edu/class/ee392o/alt_proj.pdf)
