---
title: Class 3
date: January 19, 2023
---

# Continuous Random Variable

$X$ is continuous if CDF is continuous $\forall x \in R$.

PDF defined as:

\begin{equation}
\label{eq:1} 
f_X(x) = \lim_{\Delta \rightarrow 0^+} \frac{P(x < X \leq x + \Delta)}{\Delta} 
\end{equation}

Also, 

\begin{equation}
\label{eq:2} 
P(x < X \leq x + \Delta) = F_X(x + \Delta) - F_X(x) 
\end{equation}

From, \eqref{eq:1} and \eqref{eq:2} we have

$$
f_X(x) = \frac{d F_X(x)}{dx}
$$ 

- Some Properties of PDF and CDF
- Expectation and Variance for continuous R.V.

# Transformation of Random Variables

$X \sim U(0,1)$ and $Y = e^X$

$\therefore R_X = [0, 1]$ and $R_Y = [1, e]$.

\begin{align*}
F_Y(y) &= P(Y \le y)\\
&= P(e^X \le y) = P(X \le ln(y))\\
\end{align*}

$$
\therefore
F_Y(y) = 
\begin{cases}
  0 &  y < 1\\
  ln(y) & 1 \le y \le e\\
  1 & y > e
\end{cases}
$$ 

## Method of Transform

- $X$ : continuous random variable
- $g: R \rightarrow R$
  - Assume $g$ to be strictly monotonic differentiable function
- $Y = g(X)$

$$
\therefore f_Y(y) = 
\begin{cases}
  \frac{f_X(x_1)}{|g'(x_1)|} = f_X(x_1) \left| \frac{dx_1}{dy} \right| & \text{where } g(x_1) = y\\
  0 & \text{if } g(x) = y \text{ does not have solution}
\end{cases}
$$ 
\begin{example}

Take $X$ such that $f_X(x) = 4x^3$ when $0 < x \le 1$ and $0$ otherwise.
  
Take $Y = \frac{1}{X}$ which is a strictly decreasing (hence, monotonic) function.

$$
x_1 = \frac{1}{y} \implies \frac{dx_1}{dy} = -\frac{1}{y^2}
$$ 

$$
\therefore f_Y(y) = \frac{f_X(x_1)}{|g'(x_1)|} = \frac{4x_1^3}{y^2} = 4 \left(\frac{1}{y}\right)^3 \frac{1}{y^2} = \frac{4}{y^5} \quad \text{when } y\ge 1
$$ 

\end{example}

## A General Method of Transform

- $X$ : continuous random variable
- $g: R \rightarrow R$ and $Y= g(X)$
- Partition range $R_x$ into finite intervals such that $g(x)$ is strictly monotonic and differentiable in each partition
$$
\therefore f_Y(y) = 
\begin{cases}
  \sum_{i=1}^n \frac{f_X(x_1)}{|g'(x_1)|} = \sum_{i=1}^n f_X(x_1) \left| \frac{dx_1}{dy} \right| & \text{where } g(x_1) = y\\
  0 & \text{if } g(x) = y \text{ does not have solution}
\end{cases}
$$ 

\begin{example}
Take $X \sim N(0,1)$. 
So, $f_X(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$

Now, $Y = X^2$. This is strictly decreasing in $(-\infty, 0)$ and strictly increasing in $(0, \infty)$. Therefore, we can partition the range and apply the general method of transformation.

$x^2 = y \implies x = \pm \sqrt{y}$. Let $x_1 = \sqrt{y}$ and $x_2 = -x_1$.

$$
\therefore \frac{dx_1}{dy} = \frac{1}{2 \sqrt{y}}, \ \frac{dx_2}{dy} = -\frac{1}{2\sqrt{y}}
$$ 

Thus, we obtain:

\begin{align*}
f_Y(y) &= \frac{f_X(\sqrt{y})}{|2\sqrt{y}|} + \frac{f_X(-\sqrt{y})}{|-2\sqrt{y}|}\\
&= \frac{1}{2\sqrt{2\pi y}} [e^{-\frac{y}{2}} + e^{- \frac{y}{2}}]\\
&= \frac{1}{\sqrt{2\pi y}} e^{-\frac{y}{2}} \quad R_y = (0, \infty)
\end{align*}

\end{example}

# Important Probability Distributions

- Uniform Distribution $U(a,b)$
- Normal (Gaussian) Distribution 
  - Standard Normal R.V. $Z \sim N(0,1)$
    - CDF of $Z$ denoted by $\Phi$ and it does not have a closed form solution.
  - General Normal R.V. $X \sim N(\mu, \sigma^2)$
    - Transform to standard normal using $X = \sigma Z + \mu, \ \sigma > 0$
    - PDF given as
      $$
      f_X(x) = \frac{1}{\sigma \sqrt{2\pi}} \exp \left\{ -\frac{1}{2} \left(\frac{x - \mu}{\sigma}\right)^2\right\}  
      $$ 
    
    - CDF given as $\Phi\left(\frac{x-\mu}{\sigma}\right)$.

---

# Joint R.V.

## Discrete

- Joint PMF $P_{XY} (x,y)$
  - Marginal PMF (PDF)
  
  - Joint and Marginal CDF

  - Independent when $P_{XY}(x,y) = P_X(x) P_Y(y)$. Also holds for joint CDF.

- Conditional PMF (or PDF) $P_{X|Y}(x|y)$
  - Conditional Expectation
  
  - Law of Total probability and total expectation
  $$
  E[X] = \sum_i E[X|B_i] P(B_i)
  $$
  where $B_i$ are partitions of sample space.

- Law of Unconscious Statistician

- Law of Iterated Expectation

- Conditional Variance
  $$Var(X|Y) = E[X^2|Y] - E[X|Y]^2$$

- Law of Total Variance
  $$Var(X) = E[Var(X|Y)] + Var(E[X|Y])$$

## Continuous

- Joint and Marginal PMF

- Joint and Marginal CDF
  - $F_{XY} (\infty, \infty) = 1$

  - $F_{XY} (-\infty, y) = F_{XY}(x, -\infty) = 0$

  - $P(x_1 \le X \le x_2 , \ y_1 \le Y\le y_2) = F_{XY}(x_2, y_2) - F_{XY}(x_2, y_1) - F_{XY}(x_1, y_2) + F_{XY}(x_1, y_1)$

- Conditional PDF (and CDF) and independence
  - $F_{X|Y} (x|y) = P(X\le x\ |\ Y =y)$

\begin{example}
  $X$ is continuous R.V. and event $A: a < X< b$.

  Therefore, 
  $$
F_{X|A} (x)
  \begin{cases}
1 & x >b\\
\frac{F_X(x) - F_X(a)}{F_X(b)- F_X(a)} & a\le x< b\\
0 & x < a
  \end{cases}
  $$
and
  
$$
f_{X|A}(x) = 
\begin{cases}
\frac{f_X(x)}{P(A)} & a\le x \le b\\
0 & \text{otherwise}
\end{cases}
$$ 

\end{example}

# Function of two continuous R.V.

$X,Y$ are jointly cont. R.Vs.

Let $(z,w) = g(x,y) = (g_1(x,y), g_2(x,y))$ where $g: R^2 \rightarrow R^2$ is a cont. 1-1 invertible function with cont. partial derivatives. Let $h = g^{-1}$.

$(x, y) = h(z, w) = (h_1(z,w), h_2(z,w))$

Then $z, w$ are jointly continuous and their joint PDF $f_{ZW}(z,w)$ is given by

$$
f_{ZW} (z,w) = f_{XY}(h_1 (z,w), h_2(z,w))\ |J|
$$ 

where $J$ is Jacobian, 

$$J = \begin{pmatrix}
  \frac{\partial h_1}{\partial z} & \frac{\partial h_1}{\partial w}\\
  &\\
  \frac{\partial h_2}{\partial z} & \frac{\partial h_2}{\partial w}
\end{pmatrix}$$.

\begin{example}
  $X,Y$ are independent standard normal R.V. 

$Z = 2X - Y$ and $W = - X + Y$.

$\implies X = Z + W$ and $Y = 2 W +Z$

$$
f_{XY}(x,y) = \frac{1}{2\pi} \exp \left\{ - \frac{x^2 + y^2}{2} \right\}
$$ 

$$
\begin{pmatrix}
  z\\
w
\end{pmatrix}
= 
\begin{pmatrix}
2 & -1\\
-1 & 1
\end{pmatrix}
\begin{pmatrix}
  x\\
y
\end{pmatrix}
$$

We invert this matrix and get $h(z,w)$ (we already found that when we represented $X$ and $Y$ in terms of $Z$ and $W$).

Computing $|J|$
$$
|J| = \det 
\begin{pmatrix}
1 & 1\\
1 & 2
\end{pmatrix}
 = 1
$$

$$
\therefore 
f_{ZW}(z,w) = \frac{1}{2\pi} \exp \left\{ - \frac{ (z+w)^2 + (z+2w)^2}{2} \right\}
$$ 

\end{example}

# Covariance and Correlation

## Covariance

$Cov(X, Y) = E\left[(X - E[X])(Y - E[Y])\right] = E[XY] - E[X] E[Y]$

- $Var(X) = Cov(X,X)$
- Commutative function
- $Cov(aX, Y) = a \ Cov(X,Y)$
- $Cov(X+ c, Y) = Cov(X,Y)$
- $Cov(X+ Y, Z) = Cov(X,Z) + Cov(Y,Z)$
- $Var(aX + bY) = a^2\ Var(X) + b^2 \ Var(Y) + 2ab \ Cov(X,Y)$

## Correlation

Correlation $\rho_{XY} = \rho (X, Y) = \frac{Cov(X,Y)}{\sqrt{Var(X) \ Var(Y)}}$
   
  - $-1 \leq \rho_{XY} \leq 1$.
  - If $\rho = 1$ then $Y = aX + b, \ a > 0$
  - If $\rho = -1$ then $Y = aX + b, \ a < 0$
  - $\rho (aX + b, cY +d) = \rho (X, Y)\ a, c > 0$
  - $\rho = 0$ then $X,Y$ are uncorrelated
    - For uncorrelated, $Var(X + Y) = Var(X) + Var(Y)$.
  - $\rho > 0$ then $X,Y$ are positively correlated
  - $\rho < 0$ then $X,Y$ are negatively correlated


# Bivariate Normal

$X, Y$ are said to have bivariate normal distribution with parameters $\mu_x, \sigma_x^2, \mu_y, \sigma_y^2$.

Their joint PDF is

$$
f_{XY} (x,y) = \frac{1}{2 \pi \sigma_x \sigma_y \sqrt{1 - \rho^2}} \exp \left\{ - \frac{1}{2 (1 - \rho^2)} \left[ \left( \frac{x - \mu_x}{\sigma_x} \right)^2 + \left(\frac{y - \mu_y}{\sigma_y}\right)^2 - 2\rho \frac{(x - \mu_x)(y - \mu_y)}{\sigma_x \sigma_y}\right] \right\}
$$ 
