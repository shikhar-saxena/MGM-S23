---
title: Class 9
date: February 16, 2023
---

# Recap

\begin{align*}
  D_f(p\| q) &= \int q(x) \sup_t \bc{ t \frac{p(x)}{q(x)} - f^*(t)} dx\\
  &= \sup_t \int q(x) \bc{ T_1(x) \frac{p(x)}{q(x)} - f^*(T_1(x))}\\
  &\ge \int q(x) T_1(x) \frac{p(x)}{q(x)} - f^*(T_1(x))
\end{align*}

$$
\therefore D_f(p\| q) = \sup_{T:X\ra \mathbb{R}} \int_X \bp{T(x) p(x) - f^*(T(x)) q(x)} dx
$$ 

Using parameterized family of function $T_\phi$ to approximate the optimal function $T$, can minimize an f-divergence between $p$ and $p_\theta$ (**GAN problem**) 

\begin{align*}
\theta_s &= arg\ \min_\theta \sup_\phi \bs{ E_{x\sim p} T_\phi(x) - E_{x\sim p_\theta} f^*(T_\phi(x)) }\\
&= arg\ \min_\theta \sup_\phi \bs{ E_{x\sim p} T_\phi(x) - E f^*(T_\phi(g_\theta(z))) } 
\end{align*}

where $z \ra g_\theta(z) = x$

In 2014 paper, [I. Goodfellow et al, Generative Adversarial Networks](https://arxiv.org/pdf/1406.2661.pdf)

\underline{Parameterizing:} $T_\phi(x) = \log (d_\phi(x))$

$$
\theta_f= arg\ \min_\theta \sup_\phi \bs{ E_{x\sim p} \log(d_\phi(x)) + E_{z\sim q}\log(1 - d_\phi(g_\theta(z))) } 
$$

# Algorithm for GAN

\begin{algorithm}
  \For{training iterations}{
    \For{$k$ steps}{
      Sample minibatch of $m$ noise sample $\{z^1, z^2, \ldots, z^m\}$ from priors $p_g(z)$

      Sample minibatch of $m$ examples $\{x^{(1)}, x^{(2)}, \ldots, x^{(m)}\}$ from data generating distribution $p_{data}(x)$ 
      
      Update the discriminator by ascending its stochastic gradient

      $$
      u^i \la \nabla_\phi \bp{ 1/m \sum_{i=1}^m \log d_\phi (x^i) + \log(1 - d_\phi(g_\theta(z^i)))} \quad (\phi^{i+1} \la \phi^i + \eta u^i)$$
    }

    Sample minibatch of $m$ noise samples $\{z^1, \ldots , z^m\}$
    
    Update the generator by descending 

    $$v_i = \nabla_\theta 1/m \log(1 - d_\phi (g_\theta (z^i)))\quad (\theta^{i+1} \la \theta^i - \eta v^i) $$

  }
  \caption{GAN}
\end{algorithm}


\begin{prop}
  For $G$ fixed, optimal discriminator $D$ is
  $$
  D_G^*(x) = \frac{p_{data} (x)}{p_{data}(x) + p_g(x)}
  $$ 
\end{prop}

\begin{proof}
  Recall for Two-player game
  
  $$
  \min_G \max_D V(D,G) = E_{x\sim p_{data}(x)} \log D(x) + E_{z\sim p_Z(z)} \bs{\log (1 - D(G(z)))}
  $$ 

  Therefore, for a given $G$, discriminator $D$ wants to maximize $V(G,D)$ 

  \begin{equation}
  V(G,D) = \int_x p_{data}(x) \log(D(x)) dx + p_g (x) \log(1 - D(x)) dx
  \label{eq:1}
  \end{equation}

  For any $(a,b) \in \mathbb{R}^2$ the function $f(y): y \ra a \log y + b \log (1 - y)$ achieves max in $[0,1]$ at $\frac{a }{a+b }$.

  $f'(y) = \frac{a }{y} + \frac{b (-1)}{1 - y} = 0 \implies y = \frac{a }{a + b}$

  Therefore \eqref{eq:1}, proves our proposition. 

\end{proof}

We reformulate our loss function as $\min_G C(G)$ where $C(G) = \max_D V(G, D) = V(G, D^*_G)$.

\begin{theorem}
  The global minimum of the virtual training criterion $C(G)$ is achieved if and only if
$p_g = p_{data}$. At that point, $C(G)$ achieves the value $âˆ’\log 4$.
\end{theorem}

\begin{proof}

\underline{if part}:
If $p_g = p_{data}$, $D_G^* = 1/2$. Thus, $C(G) = -\log 4$. 

\underline{only if part}:
\begin{align*}
  C(G) &= V(G, D^*_G)\\
  &= E_{x \sim p_{data}} \bs{ \log \frac{p_{data}(x)}{p_{data}(x) + p_g(x)}} + E_{x \sim p_{g}} \bs{ \log \frac{p_{g}(x)}{p_{data}(x) + p_g(x)}}\\
  &= - \log 4 + KL\bp{p_{data} \left\Vert \frac{p_{data} + p_g}{2} \right.} + KL\bp{p_{g} \left\Vert \frac{p_{data} + p_g}{2} \right.}\\
  &= - \log 4 + 2 \ JSD(p_{data}\Vert p_g) 
\end{align*}

Minimum value of $JSD$ is $0$ which is achieved when $p_g = p_{data}$.

Thus, $-\log 4$ is the best value since, $JSD \ge 0$ with equality holding only when $p_g = p_{data}$.
\end{proof}
